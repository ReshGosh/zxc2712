{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38a42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужно — раскомментируй строки ниже (и убедись, что есть интернет)\n",
    "# !pip install transformers datasets accelerate sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a05cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напиши стихотворение о весне в России.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Подготовка входного текста\n",
    "input_text = \"Напиши стихотворение о весне в России.\"\n",
    "\n",
    "# Токенизация и генерация текста\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Декодирование и вывод результата\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b2bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Напиши стихотворение о весне в России.\n",
      "model\n",
      "Весна в России – это не просто сезон, это целый праздник природы. Его начало символизирует пробуждение и оживление природы после зимней спячки. Вот стихотворение, отражающее красоту и радость весеннего времени в России:\n",
      "\n",
      "---\n",
      "\n",
      "Весна в России придёт,\n",
      "С улыбкой в сердце, словно в сказке.\n",
      "Солнце встаёт, как в небесном саду,\n",
      "И каждый цветок, словно крик, взорванный.\n",
      "\n",
      "Тишину уносит ветер весенний,\n",
      "Он шепчет о жизни, о мире, что ожил.\n",
      "И деревья, словно танцоры в лесу,\n",
      "Шепчут своим листьям весне, что она близка.\n",
      "\n",
      "Птицы возвращаются с юга в ночи,\n",
      "Они петь начали, как будто весна – это их песня.\n",
      "И лучи солнца, как жемчужины, блестят,\n",
      "На мокрой земле,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"Напиши стихотворение о весне в России.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# правильно формируем чатовый промпт\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17161a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reshg\\OneDrive\\Документы\\baselines\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\reshg\\.cache\\huggingface\\hub\\datasets--IlyaGusev--gazeta. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 60964/60964 [00:04<00:00, 13716.20 examples/s]\n",
      "Generating validation split: 100%|██████████| 6369/6369 [00:00<00:00, 28440.09 examples/s]\n",
      "Generating test split: 100%|██████████| 6793/6793 [00:00<00:00, 48590.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'На этих выходных в Берлине прошли крупные акции протеста против введенных для борьбы с коронавирусом ограничений. Демонстранты скандировали «Путин!» По словам депутата городской палаты представителей Гуннара Линдеманна («Альтернатива для Германии»), люди выкрикивали фамилию российского президента из уважения к нему. В комментарии РИА «Новости» немецкий политик отметил, что среди населения Германии Владимир Путин имеет хорошую репутацию. По его мнению, протестующие ранее пришли к российскому посольству, чтобы «привлечь внимание к условиям в Германии», надеясь, что Россия сможет оказать влияние на канцлера ФРГ Ангелу Меркель. «На мой взгляд, опасности для посольства России не возникло ни разу», — сказал депутат. Несмотря на то что протест оказался массовым, выступления носили «преимущественно мирный характер», уверен Линдеманн. По его словам, исключением стала только ситуацию у немецкого парламента. Там «несколько странных участников демонстрации попытались штурмовать бундестаг в знак протеста против правительства». Но и здесь, как считает политик, «реальной опасности не было». «Полиция не всегда реагировала соразмерно. Стали известны случаи чрезмерного насилия со стороны полиции, с которыми мы будем разбираться на парламентском уровне в берлинской палате представителей», — отметил он. В эти выходные — 29 и 30 августа — в Берлине прошли массовые акции протеста против антикоронавирусных мер: ношения масок и соблюдения безопасного расстояния в 1,5 м друг от друга. Манифестанты требуют ослабить или отменить ограничения. По данным таблоида Bild, в марше приняли участие около 30 тыс. человек. По официальным данным, манифестантов было почти в два раза меньше — 18 тыс. Протестующие начали собираться в субботу с самого утра на улице Унтер-ден-Линден. После этого они прошли маршем через центр Берлина. Финальной точкой должна была стать Улица 17-июня рядом с Брандербургскими воротами. Из-за акции протеста полиции пришлось ограничить движение по улицам Унтер-ден-Линден, Фридрихштрассе. В частности, было перекрыто пространство вокруг Бранденбургских ворот, а также Александерплац и Лейпцигерплац. Российское посольство находится по улице Унтер-ден-Линден. Именно там произошли стычки протестующих с полицией. Там же собравшиеся начали скандировать фамилию российского президента. При этом участники манифестации забрасывали полицейских бутылками и камнями. По словам представителя берлинской полиции, действия демонстрантов не были направлены против дипмиссии. «У произошедшего не было политических мотивов. На улице Унтер ден Линден находилась группа примерно в две тысячи человек, из которой в полицию бросали бутылки и камни. Два человека задержаны», — рассказала она в комментарии РИА «Новости». Позднее сенатор Берлина по вопросам внутренней политики Андреас Гайзель в прямом эфире Inforadio отметил, что часть полицейских была направлена именно к посольству России, из-за чего была ослаблена охрана бундестага. Как следствие, протестующие его чуть не захватили. «Исходили из того, что парламент, естественно, должен быть защищен. Но на улице Унтер-ден-Линден, почти перед российским посольством были насильственные действия, там кидали бутылки и камни, были раненые, задержано более 200 человек — там было необходимо полицейское подкрепление, и произошла переброска сил. Это длилось несколько минут. Это еще нуждается в оценке», — заявил сенатор. Ранее ТАСС сообщал, что всего за порядком наблюдали порядка 3 тыс. правоохранителей. Причем 1 тыс. служащих были направлены из других федеральных земель. В правоохранительных органах отмечали, что «безуспешно призывали» протестующих соблюдать положенные правила. Всего в субботу были задержаны 316 человек. Обвинения были предъявлены 131 демонстранту, 33 полицейских пострадали.', 'summary': 'Протестующие против антикоронавирусных мер немцы скандировали имя российского президента, потому что уважают его. Такое мнение выразил депутат городской палаты представителей Гуннар Линдеманн. На этих выходных в Берлине прошли крупные акции протеста. Манифестанты требовали отменить ношение масок и отказаться от соблюдения безопасного расстояния в 1,5 м друг от друга.', 'title': 'В Германии объяснили упоминание имени Путина на протестах в Берлине ', 'date': '2020-09-01 00:22:59', 'url': 'https://www.gazeta.ru/politics/2020/08/31_a_13222826.shtml'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# небольшой срез, чтобы было быстро\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"IlyaGusev/gazeta\", split=\"test[:50]\")\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91ea326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reshg\\OneDrive\\Документы\\baselines\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы сохранены:\n",
      "gazeta_train.csv → (60964, 5)\n",
      "gazeta_test.csv  → (6793, 5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"IlyaGusev/gazeta\")\n",
    "\n",
    "# HuggingFace формат → pandas\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "test_df  = ds[\"test\"].to_pandas()\n",
    "\n",
    "# сохраняем\n",
    "train_df.to_csv(\"gazeta_train.csv\", index=False)\n",
    "test_df.to_csv(\"gazeta_test.csv\", index=False)\n",
    "\n",
    "print(\"Файлы сохранены:\")\n",
    "print(\"gazeta_train.csv →\", train_df.shape)\n",
    "print(\"gazeta_test.csv  →\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce21c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reshg\\OneDrive\\Документы\\baselines\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# предполагаю, что модель и токенайзер у тебя уже загружены:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# model_name = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\u001b[39;00m\n\u001b[32m      8\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mmodel\u001b[49m.to(device)\n\u001b[32m     10\u001b[39m model.eval()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_summary\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, max_new_tokens: \u001b[38;5;28mint\u001b[39m = \u001b[32m80\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# предполагаю, что модель и токенайзер у тебя уже загружены:\n",
    "# model_name = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_summary(text: str, max_new_tokens: int = 80) -> str:\n",
    "    \"\"\"\n",
    "    Генерируем краткое резюме статьи с помощью Vikhr-Gemma.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Сделай краткое абстрактное резюме следующей русской новостной статьи \"\n",
    "                \"(1–3 предложения, выдели главное, не пиши лишних деталей):\\n\\n\"\n",
    "                + text\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,     # для метрики лучше детерминированно\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    # отрезаем префикс-промпт, чтобы не повторять вопрос\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return summary.strip()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
