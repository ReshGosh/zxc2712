{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd7bf52",
   "metadata": {},
   "source": [
    "\n",
    "# üìì –ë–ª–µ–Ω–¥ —Å–∞–±–º–∏—Ç–æ–≤ (ipynb)\n",
    "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –ø–æ–º–æ–≥–∞–µ—Ç –±–ª–µ–Ω–¥–∏—Ç—å —Å–∞–±–º–∏—Ç—ã (CSV) —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏: **mean**, **weighted**, **median**, **rank**, **logit**, **geometric**.  \n",
    "–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π (Kaggle/ODS –∏ —Ç.–ø.) —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ `id` –∏ –∫–æ–ª–æ–Ω–∫–æ–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏.\n",
    "\n",
    "**–ß—Ç–æ —É–º–µ–µ—Ç –±–ª–æ–∫–Ω–æ—Ç:**\n",
    "- –ü–æ–∏—Å–∫ –≤—Å–µ—Ö `.csv` –≤ –ø–∞–ø–∫–µ/–ø–æ —à–∞–±–ª–æ–Ω—É (–º–æ–∂–Ω–æ –∏ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤).\n",
    "- –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ID, –¥—É–±–ª–∏–∫–∞—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∏.\n",
    "- –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –±–ª–µ–Ω–¥–∞ + –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏.\n",
    "- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ `blend.csv` (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è).\n",
    "- –û—Ç—á—ë—Ç –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º –º–µ–∂–¥—É —Å–∞–±–º–∏—Ç–∞–º–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤–µ—Å–æ–≤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ecf77",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===\n",
    "# –ü–∞–ø–∫–∞ –∏–ª–∏ —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ (–º–æ–∂–Ω–æ —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π). –ü—Ä–∏–º–µ—Ä—ã:\n",
    "# PATTERN = \"submissions/*.csv\"\n",
    "# PATTERN = [\"subs/sub1.csv\", \"subs/sub2.csv\", \"subs/sub3.csv\"]\n",
    "PATTERN = \"submissions/*.csv\"\n",
    "\n",
    "# –ò–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å ID –∏ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "ID_COLUMN = \"id\"\n",
    "TARGET_COLUMN = \"prediction\"\n",
    "\n",
    "# –¢–∏–ø –±–ª–µ–Ω–¥–∞: 'mean' | 'weighted' | 'median' | 'rank' | 'logit' | 'geometric'\n",
    "BLEND_TYPE = \"mean\"\n",
    "\n",
    "# –í–µ—Å–∞ –¥–ª—è 'weighted' (–µ—Å–ª–∏ None ‚Äî –ø–æ–ø—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)\n",
    "WEIGHTS = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä: [0.5, 0.3, 0.2]\n",
    "\n",
    "# –§–∞–π–ª —Å out-of-fold/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ç–∞—Ä–≥–µ—Ç–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å –≤–µ—Å–∞)\n",
    "# –§–æ—Ä–º–∞—Ç: CSV —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ID_COLUMN –∏ 'y' (–∏—Å—Ç–∏–Ω–Ω–∞—è —Ü–µ–ª—å). –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –≤–µ—Å–∞ –∑–∞–¥–∞—Ç—å –≤—Ä—É—á–Ω—É—é.\n",
    "VALIDATION_FILE = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"oof.csv\"\n",
    "\n",
    "# –ü–∞–ø–∫–∞/–∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "OUTPUT_FILE = \"blend.csv\"\n",
    "\n",
    "# –†–µ–∂–∏–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤ –ø–µ—Ä–µ–¥ –±–ª–µ–Ω–¥–æ–º: None | 'minmax' | 'zscore'\n",
    "NORMALIZATION = None\n",
    "\n",
    "# –ù—É–∂–Ω–æ –ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "VERBOSE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985bfb3",
   "metadata": {},
   "source": [
    "## üì¶ –ò–º–ø–æ—Ä—Ç –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–≤–æ–¥–∞-–≤—ã–≤–æ–¥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def find_files(pattern):\n",
    "    if isinstance(pattern, (list, tuple)):\n",
    "        files = []\n",
    "        for p in pattern:\n",
    "            files.extend(glob.glob(p))\n",
    "        return sorted(files)\n",
    "    else:\n",
    "        return sorted(glob.glob(pattern))\n",
    "\n",
    "def read_submissions(files, id_col, target_col):\n",
    "    dfs = []\n",
    "    names = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        if id_col not in df.columns:\n",
    "            raise ValueError(f\"{f}: –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ ID '{id_col}'\")\n",
    "        if target_col not in df.columns:\n",
    "            # –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∏—Å–ª–æ–≤—É—é\n",
    "            num_cols = [c for c in df.columns if c != id_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "            if len(num_cols) == 0:\n",
    "                raise ValueError(f\"{f}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –Ω–µ—Ç '{target_col}'\")\n",
    "            if VERBOSE:\n",
    "                print(f\"[i] {f}: TARGET_COLUMN '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä—ë–º {num_cols[-1]!r}\")\n",
    "            pred_col = num_cols[-1]\n",
    "        else:\n",
    "            pred_col = target_col\n",
    "        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "        df = df[[id_col, pred_col]].copy()\n",
    "        df.rename(columns={pred_col: os.path.splitext(os.path.basename(f))[0]}, inplace=True)\n",
    "        dfs.append(df)\n",
    "        names.append(os.path.splitext(os.path.basename(f))[0])\n",
    "    # –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ merge'–∏ –ø–æ ID\n",
    "    base = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        base = base.merge(d, on=id_col, how=\"inner\", validate=\"one_to_one\")\n",
    "    return base, names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317820d8",
   "metadata": {},
   "source": [
    "## ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_and_prepare(df, id_col):\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã ID\n",
    "    dup = df[id_col].duplicated().sum()\n",
    "    if dup > 0:\n",
    "        raise ValueError(f\"–í –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã ID: {dup}\")\n",
    "    # –ü—Ä–æ–ø—É—Å–∫–∏\n",
    "    if df.isna().any().any():\n",
    "        if VERBOSE:\n",
    "            na_cnt = df.isna().sum().to_dict()\n",
    "            print('[!] –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:', na_cnt)\n",
    "        df = df.dropna().copy()\n",
    "        if VERBOSE:\n",
    "            print('[i] –ü—Ä–æ–ø—É—Å–∫–∏ —É–¥–∞–ª–µ–Ω—ã. –û—Å—Ç–∞–ª–æ—Å—å —Å—Ç—Ä–æ–∫:', len(df))\n",
    "    return df\n",
    "\n",
    "def normalize_columns(df, cols, mode=None):\n",
    "    if mode is None:\n",
    "        return df[cols].to_numpy()\n",
    "    X = df[cols].to_numpy().astype(float)\n",
    "    if mode == \"minmax\":\n",
    "        mn = X.min(axis=0, keepdims=True)\n",
    "        mx = X.max(axis=0, keepdims=True)\n",
    "        div = np.where(mx - mn == 0, 1.0, mx - mn)\n",
    "        return (X - mn) / div\n",
    "    elif mode == \"zscore\":\n",
    "        m = X.mean(axis=0, keepdims=True)\n",
    "        s = X.std(axis=0, ddof=0, keepdims=True)\n",
    "        s = np.where(s == 0, 1.0, s)\n",
    "        return (X - m) / s\n",
    "    else:\n",
    "        raise ValueError(\"NORMALIZATION –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å None | 'minmax' | 'zscore'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84e925",
   "metadata": {},
   "source": [
    "## üß™ –ú–µ—Ç–æ–¥—ã –±–ª–µ–Ω–¥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def blend_mean(X):\n",
    "    return X.mean(axis=1)\n",
    "\n",
    "def blend_median(X):\n",
    "    return np.median(X, axis=1)\n",
    "\n",
    "def blend_weighted(X, weights):\n",
    "    w = np.array(weights, dtype=float)\n",
    "    if w.ndim != 1 or w.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\"–î–ª–∏–Ω–∞ WEIGHTS –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º —Ñ–∞–π–ª–æ–≤.\")\n",
    "    w = w / w.sum()\n",
    "    return (X * w).sum(axis=1)\n",
    "\n",
    "def blend_rank(X):\n",
    "    # —Ä–∞–Ω–∂–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å—Ç–æ–ª–±–µ—Ü –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º —Ä–∞–Ω–≥–∏\n",
    "    R = np.column_stack([rankdata(X[:, j]) for j in range(X.shape[1])])\n",
    "    return R.mean(axis=1)\n",
    "\n",
    "def to_logit(p, eps=1e-9):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def from_logit(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def blend_logit(X):\n",
    "    # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ [0,1]\n",
    "    Z = np.column_stack([to_logit(X[:, j]) for j in range(X.shape[1])])\n",
    "    z = Z.mean(axis=1)\n",
    "    return from_logit(z)\n",
    "\n",
    "def blend_geometric(X, eps=1e-15):\n",
    "    Xc = np.clip(X, eps, None)\n",
    "    return np.exp(np.log(Xc).mean(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ef9d6",
   "metadata": {},
   "source": [
    "## üéØ –ü–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "def guess_task_type(y):\n",
    "    # –µ—Å–ª–∏ y –±–∏–Ω–∞—Ä–Ω–∞—è ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∏–Ω–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "    unique = np.unique(y)\n",
    "    if np.all(np.isin(unique, [0, 1])) and len(unique) <= 2:\n",
    "        return \"binary\"\n",
    "    return \"regression\"\n",
    "\n",
    "def fit_weights_with_validation(val_file, id_col, df_all, target_col_name=\"y\"):\n",
    "    val = pd.read_csv(val_file)\n",
    "    if id_col not in val.columns or target_col_name not in val.columns:\n",
    "        raise ValueError(f\"{val_file}: –Ω—É–∂–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ '{id_col}' –∏ '{target_col_name}'\")\n",
    "    data = df_all.merge(val[[id_col, target_col_name]], on=id_col, how=\"inner\")\n",
    "    X = data.drop(columns=[id_col, target_col_name]).to_numpy()\n",
    "    y = data[target_col_name].to_numpy()\n",
    "    task = guess_task_type(y)\n",
    "    if VERBOSE:\n",
    "        print(f\"[i] –ù–∞–π–¥–µ–Ω–æ {len(data)} –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –¢–∏–ø –∑–∞–¥–∞—á–∏: {task}.\")\n",
    "    # Ridge –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ (–≤–µ—Å–∞ >= 0 –º–æ–∂–Ω–æ –ø–æ–∑–∂–µ –æ–±—Ä–µ–∑–∞—Ç—å, –Ω–æ –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)\n",
    "    model = Ridge(alpha=1e-6, fit_intercept=False, positive=True)\n",
    "    model.fit(X, y)\n",
    "    w = model.coef_\n",
    "    if w.sum() == 0:\n",
    "        w = np.ones_like(w) / len(w)\n",
    "    else:\n",
    "        w = w / w.sum()\n",
    "    # –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    y_pred = X.dot(w)\n",
    "    if task == \"binary\":\n",
    "        try:\n",
    "            score = roc_auc_score(y, y_pred)\n",
    "            metric = \"AUC\"\n",
    "        except Exception:\n",
    "            score = -mean_squared_error(y, y_pred)  # fallback\n",
    "            metric = \"-MSE\"\n",
    "    else:\n",
    "        score = -mean_squared_error(y, y_pred)\n",
    "        metric = \"-MSE\"\n",
    "    if VERBOSE:\n",
    "        print(f\"[i] –ü–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ (—Å—É–º–º–∞=1): {np.round(w, 4)} | –≤–∞–ª. –º–µ—Ç—Ä–∏–∫–∞ {metric}: {score:.6f}\")\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ea28d",
   "metadata": {},
   "source": [
    "## üìà –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correlation_report(df, id_col):\n",
    "    preds = df.drop(columns=[id_col])\n",
    "    corr = preds.corr()\n",
    "    display(corr)\n",
    "    return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95e3ea",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫ –±–ª–µ–Ω–¥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) –Ω–∞–π—Ç–∏ –∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã\n",
    "files = find_files(PATTERN)\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(\"–ù–µ –Ω–∞–π–¥–µ–Ω–æ CSV –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É PATTERN.\")\n",
    "if VERBOSE:\n",
    "    print(\"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã:\")\n",
    "    for f in files:\n",
    "        print(\" -\", f)\n",
    "\n",
    "df_all, names = read_submissions(files, ID_COLUMN, TARGET_COLUMN)\n",
    "df_all = check_and_prepare(df_all, ID_COLUMN)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"[i] –†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã: {df_all.shape}\")\n",
    "    print(f\"[i] –°–∞–±–º–∏—Ç—ã: {names}\")\n",
    "\n",
    "# 2) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç\n",
    "corr = correlation_report(df_all, ID_COLUMN)\n",
    "\n",
    "# 3) –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å—Å–∏–≤–æ–≤\n",
    "cols = [c for c in df_all.columns if c != ID_COLUMN]\n",
    "X = normalize_columns(df_all, cols, NORMALIZATION)\n",
    "\n",
    "# 4) –±–ª–µ–Ω–¥\n",
    "if BLEND_TYPE == \"mean\":\n",
    "    blended = blend_mean(X)\n",
    "elif BLEND_TYPE == \"median\":\n",
    "    blended = blend_median(X)\n",
    "elif BLEND_TYPE == \"weighted\":\n",
    "    if WEIGHTS is None:\n",
    "        if VALIDATION_FILE is None:\n",
    "            raise ValueError(\"–î–ª—è 'weighted' –ª–∏–±–æ –∑–∞–¥–∞–π—Ç–µ WEIGHTS, –ª–∏–±–æ —É–∫–∞–∂–∏—Ç–µ VALIDATION_FILE –¥–ª—è –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞.\")\n",
    "        WEIGHTS = fit_weights_with_validation(VALIDATION_FILE, ID_COLUMN, df_all, target_col_name=\"y\")\n",
    "    blended = blend_weighted(X, WEIGHTS)\n",
    "elif BLEND_TYPE == \"rank\":\n",
    "    blended = blend_rank(X)\n",
    "elif BLEND_TYPE == \"logit\":\n",
    "    blended = blend_logit(X)\n",
    "elif BLEND_TYPE == \"geometric\":\n",
    "    blended = blend_geometric(X)\n",
    "else:\n",
    "    raise ValueError(\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π BLEND_TYPE. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: 'mean' | 'weighted' | 'median' | 'rank' | 'logit' | 'geometric'\")\n",
    "\n",
    "# 5) —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å\n",
    "out = df_all[[ID_COLUMN]].copy()\n",
    "out[TARGET_COLUMN] = blended\n",
    "out.to_csv(OUTPUT_FILE, index=False)\n",
    "if VERBOSE:\n",
    "    print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf02f6",
   "metadata": {},
   "source": [
    "\n",
    "## üí° –°–æ–≤–µ—Ç—ã\n",
    "- **Rank**-–±–ª–µ–Ω–¥ —É—Å—Ç–æ–π—á–∏–≤ –∫ —Ä–∞–∑–±—Ä–æ—Å—É –º–∞—Å—à—Ç–∞–±–æ–≤ –∏ —á–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ –º–µ—Ç—Ä–∏–∫–∞—Ö –Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ (AUC/MAP/NDCG).\n",
    "- **Logit**-–±–ª–µ–Ω–¥ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ [0,1].\n",
    "- **Geometric** –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏ —á–∞—Å—Ç–∏—á–Ω–æ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç log-—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ.\n",
    "- –ï—Å–ª–∏ —Å–∞–±–º–∏—Ç—ã –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ (–≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è), –≤—ã–∏–≥—Ä—ã—à –æ—Ç –±–ª–µ–Ω–¥–∞ –±—É–¥–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–º ‚Äî –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n",
    "- –î–ª—è –ø–æ–¥–±–æ—Ä–∞ –≤–µ—Å–æ–≤ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ–ª–æ–∂–∏—Ç–µ `oof.csv` —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ `id, y` –∏ –ø–æ—Å—Ç–∞–≤—å—Ç–µ `VALIDATION_FILE = \"oof.csv\"`.\n",
    "- –ï—Å–ª–∏ —É —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–Ω—ã–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ‚Äî –±–ª–æ–∫–Ω–æ—Ç —Å–∞–º –≤–æ–∑—å–º—ë—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∏—Å–ª–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É, –µ—Å–ª–∏ `TARGET_COLUMN` –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
